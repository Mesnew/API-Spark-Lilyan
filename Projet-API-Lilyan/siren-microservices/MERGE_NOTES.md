# Notes de Merge - Infrastructure Prof + APIs Ã‰tudiantes

**Date** : 2024-12-17
**Type** : Merge de l'infrastructure Scala/Spark du professeur avec les 3 APIs existantes

---

## ğŸ”„ Changements effectuÃ©s

### Infrastructure ajoutÃ©e (depuis prof)

1. **dbcli** (Dockerfile: `dbcli.Dockerfile`)
   - Conteneur Scala qui charge automatiquement les donnÃ©es CSV dans MySQL
   - ExÃ©cute `feedDB.scala` pour crÃ©er les tables et importer les donnÃ©es
   - Se termine aprÃ¨s avoir chargÃ© les donnÃ©es (`condition: service_completed_successfully`)

2. **spark** (Dockerfile: `analyticcli.Dockerfile`)
   - Serveur Spark Connect sur le port 15002
   - Permet les analyses distribuÃ©es via Spark
   - ExÃ©cute `analyticcli.scala`

3. **Fichiers d'infrastructure**
   - `build.sbt` : Configuration du projet Scala
   - `project/` : Configuration SBT
   - `src/main/scala/` : Code source Scala
   - `my.cnf` : Configuration MySQL optimisÃ©e

### Services conservÃ©s (APIs Ã©tudiantes)

1. **oauth2** (Port 3000)
   - API OAuth2 (Node.js + Express)
   - Authentification pour les 2 autres APIs

2. **api-mysql** (Port 3001)
   - API de recherche d'entreprises (Python FastAPI)
   - Endpoints : SIREN, code activitÃ©, nom

3. **api-spark** (Port 3002)
   - API de statistiques (Node.js + Express)
   - **NOUVEAU** : Variable d'environnement `SPARK_CONNECT_URL` ajoutÃ©e
   - Peut maintenant se connecter au vrai Spark Connect

---

## ğŸ“Š Architecture finale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE COMPLÃˆTE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         Client (Postman / Browser)
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
        v                       v
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ OAuth2  â”‚           â”‚  API MySQL  â”‚
   â”‚ :3000   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  :3001      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚
        â”‚                       â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
   â”‚ API Spark  â”‚               â”‚
   â”‚ :3002      â”‚               â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜               â”‚
         â”‚                      â”‚
         â”‚                      â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚                                 â”‚
    v                                 v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  MySQL  â”‚
â”‚ :15002  â”‚                     â”‚ :3367   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â–²                                â”‚
    â”‚                                â”‚
    â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚           â”‚
    â”‚       â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚       â”‚ dbcli  â”‚ (one-shot: loads CSV data)
    â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â””â”€â”€â”€ analyticcli (Spark Connect Server)
```

---

## ğŸ”§ Changements dans docker-compose.yaml

### Ordre de dÃ©marrage

1. **db** (MySQL) - DÃ©marre en premier
2. **dbcli** - Charge les donnÃ©es une fois MySQL prÃªt
3. **spark** - DÃ©marre aprÃ¨s dbcli (donnÃ©es chargÃ©es)
4. **oauth2** - DÃ©marre aprÃ¨s dbcli
5. **api-mysql** - DÃ©marre aprÃ¨s db + dbcli + oauth2
6. **api-spark** - DÃ©marre aprÃ¨s db + dbcli + spark + oauth2

### DÃ©pendances ajoutÃ©es

- Tous les services attendent `dbcli: condition: service_completed_successfully`
- `api-spark` dÃ©pend maintenant de `spark: condition: service_started`

---

## ğŸ”‘ Changements dans .env

**Ancien (dÃ©veloppement)** :
```
MYSQL_ROOT_PASSWORD=Dev_Root_Pass_2024!
MYSQL_PASSWORD=Dev_Siren_Pass_2024!
MYSQL_PORT=3366
```

**Nouveau (compatibilitÃ© prof)** :
```
MYSQL_ROOT_PASSWORD=12345678
MYSQL_PASSWORD=12345678
MYSQL_HOST=db
MYSQL_PORT=3306
```

**âš ï¸ IMPORTANT** : Les credentials ont Ã©tÃ© simplifiÃ©s pour correspondre Ã  ceux du prof.

---

## ğŸ“¦ DonnÃ©es

Le projet utilise maintenant **deux mÃ©thodes** de chargement :

1. **init-db.sql** (ancienne mÃ©thode)
   - Fichier SQL avec 20 entreprises de test
   - UtilisÃ© manuellement si besoin

2. **dbcli + feedDB.scala** (nouvelle mÃ©thode - AUTOMATIQUE)
   - Charge automatiquement le fichier CSV complet
   - ExÃ©cutÃ© au dÃ©marrage du conteneur
   - Fichier attendu : `data/StockUniteLegale_utf8.csv`

### TÃ©lÃ©charger les donnÃ©es

```bash
cd data
wget https://object.files.data.gouv.fr/data-pipeline-open/siren/stock/StockUniteLegale_utf8.zip
unzip StockUniteLegale_utf8.zip
```

---

## ğŸš€ DÃ©marrage

```bash
# TÃ©lÃ©charger les donnÃ©es (si pas dÃ©jÃ  fait)
cd data
wget https://object.files.data.gouv.fr/data-pipeline-open/siren/stock/StockUniteLegale_utf8.zip
unzip StockUniteLegale_utf8.zip
cd ..

# Lancer tous les services
docker-compose build
docker-compose up

# Voir les logs
docker-compose logs -f

# VÃ©rifier que tout fonctionne
docker ps
```

---

## ğŸ§ª Tests

### 1. Obtenir un token OAuth2

```bash
curl -X POST http://localhost:3000/oauth/token \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "grant_type=password" \
  -d "username=user1" \
  -d "password=DevUser1Pass2024!" \
  -d "client_id=client-app" \
  -d "client_secret=Dev_Client_Secret_2024!"
```

### 2. Tester les APIs

```bash
TOKEN="votre_token_ici"

# API MySQL
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:3001/entreprises/siren/123456789

# API Spark (statistiques)
curl -H "Authorization: Bearer $TOKEN" \
  "http://localhost:3002/stats/activites/count?page=1&limit=5"
```

### 3. VÃ©rifier Spark Connect

```bash
# Logs du serveur Spark
docker logs spark

# Le port 15002 doit Ãªtre accessible
nc -zv localhost 15002
```

---

## ğŸ“ TODO pour api-spark

L'API Spark peut maintenant se connecter au vrai Spark Connect via `SPARK_CONNECT_URL=spark:15002`.

**Modifications suggÃ©rÃ©es dans `services/api-spark/app.js`** :

```javascript
// Remplacer les requÃªtes MySQL directes par Spark Connect
// Exemple avec spark-connect-client (Ã  installer)

const { SparkSession } = require('@apache/spark-connect');

const spark = SparkSession.builder()
  .remote(process.env.SPARK_CONNECT_URL || 'localhost:15002')
  .build();

// Lire depuis MySQL via Spark
const df = spark.read
  .format('jdbc')
  .option('url', `jdbc:mysql://${process.env.MYSQL_HOST}:3306/${process.env.MYSQL_DATABASE}`)
  .option('driver', 'com.mysql.cj.jdbc.Driver')
  .option('dbtable', 'unite_legale')
  .option('user', process.env.MYSQL_USER)
  .option('password', process.env.MYSQL_PASSWORD)
  .load();

// Effectuer des analyses distribuÃ©es
const stats = df.groupBy('activite_principale_unite_legale')
  .count()
  .orderBy('count', { ascending: false });
```

---

## ğŸ”— Liens utiles

- **OAuth2 Swagger** : http://localhost:3000/api-docs
- **API MySQL Swagger** : http://localhost:3001/docs
- **API Spark Swagger** : http://localhost:3002/api-docs
- **Spark Connect** : localhost:15002

---

## ğŸ¯ RÃ©sumÃ©

**Avant** : 3 APIs + MySQL (donnÃ©es chargÃ©es manuellement)
**AprÃ¨s** : 3 APIs + MySQL + Spark Connect + chargement automatique des donnÃ©es

**Avantages** :
- âœ… Chargement automatique des donnÃ©es CSV
- âœ… Infrastructure Spark Connect disponible
- âœ… Toutes les APIs conservÃ©es et fonctionnelles
- âœ… Architecture complÃ¨te et conteneurisÃ©e

**Prochaines Ã©tapes** :
- [ ] Modifier `api-spark` pour utiliser Spark Connect au lieu de MySQL
- [ ] Tester les performances avec le dataset complet
- [ ] ImplÃ©menter des analyses distribuÃ©es avec Spark
